{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gauravgurjar/rent-contracts-dubai?scriptVersionId=224390180\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/gauravgurjar/rent-contracts-dubai?scriptVersionId=224066437\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"markdown","source":"# Data Loading\n---\nInstalling the libraries","metadata":{"id":"3bjBS23sKI7l"}},{"cell_type":"code","source":"%%bash\ndownload_latest_file() {\n    current_date=$(date +%Y-%m-%d)\n    base_url=\"https://github.com/ggurjar333/real-estate-analysis-dubai/releases/download\"\n    release=\"release-${current_date}\"\n    file=\"dld_rent_contracts_${current_date}.parquet\"\n\n    wget -q \"${base_url}/${release}/${file}\" # Added -q option to make wget quiet. \n}\ndownload_latest_file","metadata":{"id":"SQ-7C1JOrg-m","outputId":"c6d112f6-f42c-473d-b1e3-dbd6c77b1564","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:32:08.965878Z","iopub.execute_input":"2025-02-25T09:32:08.966231Z","iopub.status.idle":"2025-02-25T09:32:12.079242Z","shell.execute_reply.started":"2025-02-25T09:32:08.966197Z","shell.execute_reply":"2025-02-25T09:32:12.078154Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Installing required packages\n!pip install pyspark\n!pip install findspark\n!pip install pandas\n\nimport findspark\nfindspark.init()\n\nimport pandas as pd\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\n# Creating a spark session\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Dubai Land Development\") \\\n    .getOrCreate()","metadata":{"id":"HOu3m8VnPPdD","outputId":"0bab0ec1-5cff-4ffa-c9ed-8325a78521e6","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:32:12.080202Z","iopub.execute_input":"2025-02-25T09:32:12.080485Z","iopub.status.idle":"2025-02-25T09:32:33.744376Z","shell.execute_reply.started":"2025-02-25T09:32:12.080461Z","shell.execute_reply":"2025-02-25T09:32:33.743145Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\nCollecting findspark\n  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\nDownloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\nInstalling collected packages: findspark\nSuccessfully installed findspark-2.0.1\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datetime import date","metadata":{"id":"2rNexZLht5fg","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:32:33.745829Z","iopub.execute_input":"2025-02-25T09:32:33.746484Z","iopub.status.idle":"2025-02-25T09:32:33.751059Z","shell.execute_reply.started":"2025-02-25T09:32:33.746452Z","shell.execute_reply":"2025-02-25T09:32:33.749854Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"rent_contracts_df = spark.read.parquet(f\"dld_rent_contracts_{date.today()}.parquet\")\nrent_contracts_df.printSchema()","metadata":{"id":"ZlhUkzIbpAMe","outputId":"8f1eeff0-82b2-4a31-e429-671446b5f177","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:32:33.753541Z","iopub.execute_input":"2025-02-25T09:32:33.753873Z","iopub.status.idle":"2025-02-25T09:32:38.714542Z","shell.execute_reply.started":"2025-02-25T09:32:33.753847Z","shell.execute_reply":"2025-02-25T09:32:38.713476Z"}},"outputs":[{"name":"stdout","text":"root\n |-- contract_id: string (nullable = true)\n |-- contract_reg_type_id: long (nullable = true)\n |-- contract_reg_type_ar: string (nullable = true)\n |-- contract_reg_type_en: string (nullable = true)\n |-- contract_start_date: string (nullable = true)\n |-- contract_end_date: string (nullable = true)\n |-- contract_amount: long (nullable = true)\n |-- annual_amount: long (nullable = true)\n |-- no_of_prop: long (nullable = true)\n |-- line_number: long (nullable = true)\n |-- is_free_hold: long (nullable = true)\n |-- ejari_bus_property_type_id: long (nullable = true)\n |-- ejari_bus_property_type_ar: string (nullable = true)\n |-- ejari_bus_property_type_en: string (nullable = true)\n |-- ejari_property_type_id: long (nullable = true)\n |-- ejari_property_type_en: string (nullable = true)\n |-- ejari_property_type_ar: string (nullable = true)\n |-- ejari_property_sub_type_id: long (nullable = true)\n |-- ejari_property_sub_type_en: string (nullable = true)\n |-- ejari_property_sub_type_ar: string (nullable = true)\n |-- property_usage_en: string (nullable = true)\n |-- property_usage_ar: string (nullable = true)\n |-- project_number: long (nullable = true)\n |-- project_name_ar: string (nullable = true)\n |-- project_name_en: string (nullable = true)\n |-- master_project_ar: string (nullable = true)\n |-- master_project_en: string (nullable = true)\n |-- area_id: long (nullable = true)\n |-- area_name_ar: string (nullable = true)\n |-- area_name_en: string (nullable = true)\n |-- actual_area: long (nullable = true)\n |-- nearest_landmark_ar: string (nullable = true)\n |-- nearest_landmark_en: string (nullable = true)\n |-- nearest_metro_ar: string (nullable = true)\n |-- nearest_metro_en: string (nullable = true)\n |-- nearest_mall_ar: string (nullable = true)\n |-- nearest_mall_en: string (nullable = true)\n |-- tenant_type_id: long (nullable = true)\n |-- tenant_type_ar: string (nullable = true)\n |-- tenant_type_en: string (nullable = true)\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"rent_contracts_df.show(5)","metadata":{"id":"pzexUGZQpXU2","outputId":"f19903f5-fb56-4285-9072-3b5634e49795","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:32:38.716041Z","iopub.execute_input":"2025-02-25T09:32:38.716359Z","iopub.status.idle":"2025-02-25T09:32:42.775298Z","shell.execute_reply.started":"2025-02-25T09:32:38.716323Z","shell.execute_reply":"2025-02-25T09:32:42.773439Z"}},"outputs":[{"name":"stdout","text":"+-------------+--------------------+--------------------+--------------------+-------------------+-----------------+---------------+-------------+----------+-----------+------------+--------------------------+--------------------------+--------------------------+----------------------+----------------------+----------------------+--------------------------+--------------------------+--------------------------+-----------------+-----------------+--------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+---------------+---------------+--------------+--------------+--------------+\n|  contract_id|contract_reg_type_id|contract_reg_type_ar|contract_reg_type_en|contract_start_date|contract_end_date|contract_amount|annual_amount|no_of_prop|line_number|is_free_hold|ejari_bus_property_type_id|ejari_bus_property_type_ar|ejari_bus_property_type_en|ejari_property_type_id|ejari_property_type_en|ejari_property_type_ar|ejari_property_sub_type_id|ejari_property_sub_type_en|ejari_property_sub_type_ar|property_usage_en|property_usage_ar|project_number|     project_name_ar|     project_name_en|   master_project_ar|   master_project_en|area_id|        area_name_ar|        area_name_en|actual_area| nearest_landmark_ar| nearest_landmark_en|    nearest_metro_ar|    nearest_metro_en|nearest_mall_ar|nearest_mall_en|tenant_type_id|tenant_type_ar|tenant_type_en|\n+-------------+--------------------+--------------------+--------------------+-------------------+-----------------+---------------+-------------+----------+-----------+------------+--------------------------+--------------------------+--------------------------+----------------------+----------------------+----------------------+--------------------------+--------------------------+--------------------------+-----------------+-----------------+--------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+---------------+---------------+--------------+--------------+--------------+\n|CRT1012981266|                   1|                جديد|                 New|         07-04-2019|       06-04-2020|          85000|        85000|         1|          1|           1|                         2|                      وحدة|                      Unit|                     2|                Office|                  مكتب|                       422|                    Office|                      مكتب|       Commercial|            تجاري|           467|        إمباير هايتس|      EMPIRE HEIGHTS|      الخليج التجاري|        Business Bay|    526|      الخليج التجارى|        Business Bay|        140|       وسط مدينة دبي|      Downtown Dubai|محطة مترو بوج خلي...|Buj Khalifa Dubai...|        مول دبي|     Dubai Mall|             1|           شخص|        Person|\n|CRT1012983196|                   1|                جديد|                 New|         20-04-2019|       19-04-2020|         110000|       110000|         1|          1|           1|                         4|                      فيلا|                     Villa|                   841|                 Villa|                  فيلا|                         2|          2 bed rooms+hall|             غرفتين و صالة|      Residential|             سكني|          NULL|                    |                    |  قرية جميرا المثلثة|Jumeirah Village ...|    442|البرشاء جنوب الخامسة|Al Barsha South F...|        734|أكاديمية المدينة ...|Sports City Swimm...|    محطة مترو النخيل|Nakheel Metro Sta...|     مارينا مول|    Marina Mall|             1|           شخص|        Person|\n|CRT1012984226|                   1|                جديد|                 New|         11-04-2019|       10-04-2020|         100000|       100000|         1|          1|           1|                         4|                      فيلا|                     Villa|                   841|                 Villa|                  فيلا|                         3|          3 bed rooms+hall|         ثلاثة غرفة و صالة|      Residential|             سكني|          1488|ريم - ميرا أوسيس ...|REEM - MIRA OASIS...|                    |                    |    506|           اليلايس 1|       Al Yelayiss 1|        324|   دورة دبي للدراجات|Dubai Cycling Course|                    |                    |               |               |             1|           شخص|        Person|\n|CRT1012984996|                   2|               تجديد|               Renew|         18-03-2019|       17-03-2020|         150000|       150000|         1|          1|           1|                         4|                      فيلا|                     Villa|                   841|                 Villa|                  فيلا|                         3|          3 bed rooms+hall|         ثلاثة غرفة و صالة|      Residential|             سكني|          1377|أرابيان رانشز - م...|ARABIAN RANCHES -...|المرابع العربية 2...|Arabian Ranches I...|    463|        وادي الصفا 7|      Wadi Al Safa 7|        405|          موتور سيتي|          Motor City|                    |                    |               |               |             1|           شخص|        Person|\n|CRT1012986616|                   1|                جديد|                 New|         15-04-2019|       14-04-2020|          95000|        95000|         1|          1|           1|                         2|                      وحدة|                      Unit|                   842|                  Flat|                   شقه|                         1|            1bed room+Hall|               غرفة و صالة|      Residential|             سكني|          NULL|                    |                    |جميرا بيتش ريزيدن...|Jumeriah Beach Re...|    330|            مرسى دبي|         Marsa Dubai|        103|           برج العرب|        Burj Al Arab|    مساكن شاطئ جميرا|Jumeirah Beach Re...|     مارينا مول|    Marina Mall|          NULL|              |              |\n+-------------+--------------------+--------------------+--------------------+-------------------+-----------------+---------------+-------------+----------+-----------+------------+--------------------------+--------------------------+--------------------------+----------------------+----------------------+----------------------+--------------------------+--------------------------+--------------------------+-----------------+-----------------+--------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+---------------+---------------+--------------+--------------+--------------+\nonly showing top 5 rows\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from pyspark.sql.functions import col\n# Convert contract start and end dates to datetime objects if they aren't already\nrent_contracts_df = rent_contracts_df.withColumn(\"contract_start_date\", col(\"contract_start_date\").cast(\"date\"))\nrent_contracts_df = rent_contracts_df.withColumn(\"contract_end_date\", col(\"contract_end_date\").cast(\"date\"))\n","metadata":{"id":"NefwqDw0vvyh","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:32:42.776146Z","iopub.execute_input":"2025-02-25T09:32:42.77648Z","iopub.status.idle":"2025-02-25T09:32:43.020212Z","shell.execute_reply.started":"2025-02-25T09:32:42.776446Z","shell.execute_reply":"2025-02-25T09:32:43.01913Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# prompt: 2. Descriptive Statistics\n# Contract Amounts: Calculate the average, median, and range of contract_amount to understand pricing trends.\n# Contract Duration: Analyze the duration of contracts by calculating the difference between contract_end_date and contract_start_date.\n# Property Types: Count the occurrences of each property type (ejari_property_type_en) to identify the most common types of properties rented.\n\nfrom pyspark.sql.functions import avg, max, min, col, datediff, count\n\n# Calculate average, median, and range of contract_amount\ncontract_amount_stats = rent_contracts_df.select(\n    avg(\"contract_amount\").alias(\"avg_contract_amount\"),\n    max(\"contract_amount\").alias(\"max_contract_amount\"),\n    min(\"contract_amount\").alias(\"min_contract_amount\")\n)\n\ncontract_amount_stats.show()\n\n# Calculate contract duration\nrent_contracts_df = rent_contracts_df.withColumn(\n    \"contract_duration\", datediff(col(\"contract_end_date\"), col(\"contract_start_date\"))\n)\nrent_contracts_df.show(5)\n\n# Count occurrences of each property type\nproperty_type_counts = rent_contracts_df.groupBy(\"ejari_property_type_en\").agg(\n    count(\"*\").alias(\"property_count\")\n)\n\nproperty_type_counts.show()\n","metadata":{"id":"C2i04JiXttXJ","outputId":"c8fef5b6-11b2-4af3-c02b-ba1ec2f1e223","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:32:43.021396Z","iopub.execute_input":"2025-02-25T09:32:43.021862Z","iopub.status.idle":"2025-02-25T09:32:47.202135Z","shell.execute_reply.started":"2025-02-25T09:32:43.021819Z","shell.execute_reply":"2025-02-25T09:32:47.19599Z"}},"outputs":[{"name":"stdout","text":"+-------------------+-------------------+-------------------+\n|avg_contract_amount|max_contract_amount|min_contract_amount|\n+-------------------+-------------------+-------------------+\n|  731661.5617702793|         4200000003|                  0|\n+-------------------+-------------------+-------------------+\n\n+-------------+--------------------+--------------------+--------------------+-------------------+-----------------+---------------+-------------+----------+-----------+------------+--------------------------+--------------------------+--------------------------+----------------------+----------------------+----------------------+--------------------------+--------------------------+--------------------------+-----------------+-----------------+--------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+---------------+---------------+--------------+--------------+--------------+-----------------+\n|  contract_id|contract_reg_type_id|contract_reg_type_ar|contract_reg_type_en|contract_start_date|contract_end_date|contract_amount|annual_amount|no_of_prop|line_number|is_free_hold|ejari_bus_property_type_id|ejari_bus_property_type_ar|ejari_bus_property_type_en|ejari_property_type_id|ejari_property_type_en|ejari_property_type_ar|ejari_property_sub_type_id|ejari_property_sub_type_en|ejari_property_sub_type_ar|property_usage_en|property_usage_ar|project_number|     project_name_ar|     project_name_en|   master_project_ar|   master_project_en|area_id|        area_name_ar|        area_name_en|actual_area| nearest_landmark_ar| nearest_landmark_en|    nearest_metro_ar|    nearest_metro_en|nearest_mall_ar|nearest_mall_en|tenant_type_id|tenant_type_ar|tenant_type_en|contract_duration|\n+-------------+--------------------+--------------------+--------------------+-------------------+-----------------+---------------+-------------+----------+-----------+------------+--------------------------+--------------------------+--------------------------+----------------------+----------------------+----------------------+--------------------------+--------------------------+--------------------------+-----------------+-----------------+--------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+---------------+---------------+--------------+--------------+--------------+-----------------+\n|CRT1012981266|                   1|                جديد|                 New|               NULL|             NULL|          85000|        85000|         1|          1|           1|                         2|                      وحدة|                      Unit|                     2|                Office|                  مكتب|                       422|                    Office|                      مكتب|       Commercial|            تجاري|           467|        إمباير هايتس|      EMPIRE HEIGHTS|      الخليج التجاري|        Business Bay|    526|      الخليج التجارى|        Business Bay|        140|       وسط مدينة دبي|      Downtown Dubai|محطة مترو بوج خلي...|Buj Khalifa Dubai...|        مول دبي|     Dubai Mall|             1|           شخص|        Person|             NULL|\n|CRT1012983196|                   1|                جديد|                 New|               NULL|             NULL|         110000|       110000|         1|          1|           1|                         4|                      فيلا|                     Villa|                   841|                 Villa|                  فيلا|                         2|          2 bed rooms+hall|             غرفتين و صالة|      Residential|             سكني|          NULL|                    |                    |  قرية جميرا المثلثة|Jumeirah Village ...|    442|البرشاء جنوب الخامسة|Al Barsha South F...|        734|أكاديمية المدينة ...|Sports City Swimm...|    محطة مترو النخيل|Nakheel Metro Sta...|     مارينا مول|    Marina Mall|             1|           شخص|        Person|             NULL|\n|CRT1012984226|                   1|                جديد|                 New|               NULL|             NULL|         100000|       100000|         1|          1|           1|                         4|                      فيلا|                     Villa|                   841|                 Villa|                  فيلا|                         3|          3 bed rooms+hall|         ثلاثة غرفة و صالة|      Residential|             سكني|          1488|ريم - ميرا أوسيس ...|REEM - MIRA OASIS...|                    |                    |    506|           اليلايس 1|       Al Yelayiss 1|        324|   دورة دبي للدراجات|Dubai Cycling Course|                    |                    |               |               |             1|           شخص|        Person|             NULL|\n|CRT1012984996|                   2|               تجديد|               Renew|               NULL|             NULL|         150000|       150000|         1|          1|           1|                         4|                      فيلا|                     Villa|                   841|                 Villa|                  فيلا|                         3|          3 bed rooms+hall|         ثلاثة غرفة و صالة|      Residential|             سكني|          1377|أرابيان رانشز - م...|ARABIAN RANCHES -...|المرابع العربية 2...|Arabian Ranches I...|    463|        وادي الصفا 7|      Wadi Al Safa 7|        405|          موتور سيتي|          Motor City|                    |                    |               |               |             1|           شخص|        Person|             NULL|\n|CRT1012986616|                   1|                جديد|                 New|               NULL|             NULL|          95000|        95000|         1|          1|           1|                         2|                      وحدة|                      Unit|                   842|                  Flat|                   شقه|                         1|            1bed room+Hall|               غرفة و صالة|      Residential|             سكني|          NULL|                    |                    |جميرا بيتش ريزيدن...|Jumeriah Beach Re...|    330|            مرسى دبي|         Marsa Dubai|        103|           برج العرب|        Burj Al Arab|    مساكن شاطئ جميرا|Jumeirah Beach Re...|     مارينا مول|    Marina Mall|          NULL|              |              |             NULL|\n+-------------+--------------------+--------------------+--------------------+-------------------+-----------------+---------------+-------------+----------+-----------+------------+--------------------------+--------------------------+--------------------------+----------------------+----------------------+----------------------+--------------------------+--------------------------+--------------------------+-----------------+-----------------+--------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+---------------+---------------+--------------+--------------+--------------+-----------------+\nonly showing top 5 rows\n\n+----------------------+--------------+\n|ejari_property_type_en|property_count|\n+----------------------+--------------+\n|               Parking|          4013|\n|           Health club|           983|\n|                  Bank|           178|\n|    Resturants Complex|           480|\n|        Medical center|           944|\n|                  Farm|            49|\n|                   Spa|           108|\n|                   ATM|           269|\n|     Complex Warehouse|          2733|\n|            Open space|          2702|\n|         Land Parking |            38|\n|                  Hall|            22|\n|                Office|       1168738|\n|  Supermarket, a mu...|           290|\n|                School|           640|\n|           Supermarket|           312|\n|                  Flat|       4516996|\n|        Complex Villas|         53474|\n|               Nursery|           330|\n|      Hotel apartments|         18389|\n+----------------------+--------------+\nonly showing top 20 rows\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# prompt: 3. Trends Over Time\n# Contract Start and End Dates: Analyze the distribution of contract start dates to identify peak rental periods.\n# Renewals vs. New Contracts: Compare the number of new contracts versus renewals to assess tenant retention and market stability.\n\nfrom pyspark.sql.functions import month, year\n\n# Analyze contract start date distribution\nstart_date_counts = rent_contracts_df.groupBy(month(\"contract_start_date\"), year(\"contract_start_date\")).count().orderBy(year(\"contract_start_date\"),month(\"contract_start_date\"))\nstart_date_counts.show(5)\n\n\n# Analyze contract renewal vs. new contracts (assuming you have a column indicating this)\n#  Replace \"is_renewal\" with the actual column name in your DataFrame.\n# If there's no such column, you need to engineer it based on your data.\nif \"contract_reg_type_en\" in rent_contracts_df.columns:\n    reg_type_counts = rent_contracts_df.groupBy(\"contract_reg_type_en\").count()\n    reg_type_counts.show()\nelse:\n    print(\"No 'contract_reg_type_en' column found. Please add a column to identify contract renewals.\")\n","metadata":{"id":"09hc1geQuimB","outputId":"43afbeb7-e12f-4d94-e1d2-62c0e362a83c","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:32:47.203062Z","iopub.execute_input":"2025-02-25T09:32:47.203433Z","iopub.status.idle":"2025-02-25T09:32:50.973223Z","shell.execute_reply.started":"2025-02-25T09:32:47.203397Z","shell.execute_reply":"2025-02-25T09:32:50.969891Z"}},"outputs":[{"name":"stdout","text":"+--------------------------+-------------------------+-------+\n|month(contract_start_date)|year(contract_start_date)|  count|\n+--------------------------+-------------------------+-------+\n|                      NULL|                     NULL|8559440|\n+--------------------------+-------------------------+-------+\n\n+--------------------+-------+\n|contract_reg_type_en|  count|\n+--------------------+-------+\n|               Renew|4094747|\n|                 New|4464693|\n+--------------------+-------+\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# prompt: 4. Property Usage Analysis\n# Residential vs. Commercial: Compare the average contract amounts for residential and commercial properties to understand market dynamics.\n# Property Subtypes: Analyze the distribution of property subtypes (e.g., 1 bed room, 2 bed rooms) to identify popular configurations.\n\n# Analyze residential vs. commercial property contract amounts\nresidential_commercial_avg = rent_contracts_df.groupBy(\"ejari_property_type_en\").agg(\n    avg(\"contract_amount\").alias(\"avg_contract_amount\")\n)\nresidential_commercial_avg.show()\n\n# Analyze the distribution of property subtypes (assuming you have a 'property_subtype' column)\nif \"ejari_property_sub_type_en\" in rent_contracts_df.columns:\n    property_subtype_counts = rent_contracts_df.groupBy(\"ejari_property_sub_type_en\").agg(\n        count(\"*\").alias(\"property_subtype_count\")\n    )\n    property_subtype_counts.show()\nelse:\n    print(\"No 'ejari_property_sub_type_en' column found. Please add a relevant column to your DataFrame.\")\n","metadata":{"id":"m6WglkVuyVh1","outputId":"b83cf8b6-446d-47d3-cc87-f0afa1749058","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:32:50.977016Z","iopub.execute_input":"2025-02-25T09:32:50.97919Z","iopub.status.idle":"2025-02-25T09:32:53.838624Z","shell.execute_reply.started":"2025-02-25T09:32:50.979139Z","shell.execute_reply":"2025-02-25T09:32:53.837502Z"}},"outputs":[{"name":"stdout","text":"+----------------------+-------------------+\n|ejari_property_type_en|avg_contract_amount|\n+----------------------+-------------------+\n|               Parking|  457364.4467979068|\n|           Health club| 1563110.4343845372|\n|                  Bank|  3337451.146067416|\n|    Resturants Complex|  813715.2333333333|\n|        Medical center| 1151954.0466101696|\n|                  Farm|  637142.8571428572|\n|                   Spa|  643067.9722222222|\n|                   ATM|  61935.57249070632|\n|     Complex Warehouse| 195434.60373216245|\n|            Open space| 464621.38119911175|\n|         Land Parking | 147998.73684210525|\n|                  Hall| 387615.95454545453|\n|                Office|  304415.0374840212|\n|  Supermarket, a mu...|  3225602.027586207|\n|                School|  1.0064177715625E7|\n|           Supermarket| 2226240.6923076925|\n|                  Flat|  590509.2390139375|\n|        Complex Villas|  168764.4980364289|\n|               Nursery|  915195.9939393939|\n|      Hotel apartments|   6202819.87862309|\n+----------------------+-------------------+\nonly showing top 20 rows\n\n+--------------------------+----------------------+\n|ejari_property_sub_type_en|property_subtype_count|\n+--------------------------+----------------------+\n|      Showroom with mez...|                  5231|\n|         10 bed rooms+hall|                  1169|\n|                   Parking|                  4092|\n|          Portacabin Rooms|                 11315|\n|               Health club|                   971|\n|                      Bank|                   189|\n|          4 bed rooms+hall|                205156|\n|                      Farm|                    34|\n|                       Spa|                   108|\n|                       ATM|                   287|\n|                Open space|                   680|\n|                    Office|               1154683|\n|                    Studio|                895095|\n|      Coffe shop,Fast Food|                   117|\n|                   Laundry|                    59|\n|                Hat Saloon|                   755|\n|                    School|                   534|\n|               Supermarket|                  3074|\n|                Sign board|                    96|\n|                   Nursery|                   670|\n+--------------------------+----------------------+\nonly showing top 20 rows\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# 5. Geographic Insights\n# Area Analysis: Group data by area_name_en to identify which areas have the highest number of contracts and average contract amounts.\n# Proximity to Landmarks: Analyze how proximity to landmarks (e.g., malls, metro stations) affects rental prices.\n# nearest_landmark_en, nearest_metro_en, nearest_mall_en\narea_analysis = rent_contracts_df.groupBy(\"area_name_en\").agg(\n    count(\"*\").alias(\"contract_count\"),\n    avg(\"contract_amount\").alias(\"avg_contract_amount\")\n)\narea_analysis.show()\n\nproximity_analysis = rent_contracts_df.groupBy(\"nearest_landmark_en\", \"nearest_metro_en\", \"nearest_mall_en\").agg(\n    avg(\"contract_amount\").alias(\"avg_contract_amount\")\n)\nproximity_analysis.show()","metadata":{"id":"Tf2mh1fczTFR","outputId":"1e1cf61a-6535-446a-f750-6d74c6d0f823","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:32:53.839563Z","iopub.execute_input":"2025-02-25T09:32:53.839982Z","iopub.status.idle":"2025-02-25T09:32:59.385854Z","shell.execute_reply.started":"2025-02-25T09:32:53.839945Z","shell.execute_reply":"2025-02-25T09:32:59.38477Z"}},"outputs":[{"name":"stdout","text":"+--------------------+--------------+-------------------+\n|        area_name_en|contract_count|avg_contract_amount|\n+--------------------+--------------+-------------------+\n|    Um Hurair Second|         20521|  379538.1990156425|\n|         Al Khabeesi|         53918| 104672.02577988798|\n|        Al Rashidiya|         19632| 101951.47376731866|\n|Al Barsha South F...|         20882|   107567.288430227|\n|          Al Kheeran|         15352|  704012.8445153725|\n|       Al Goze Third|        132369|  2099384.749782804|\n|      Al Twar Second|           747|  301633.3614457831|\n|             Al Ttay|         13335|  900357.6253468316|\n|     Um Suqaim First|         13624|  631046.4059013505|\n|  Nad Al Shiba First|         11799| 184870.52894313078|\n|       Lehbab Second|           919| 46045.532100108816|\n|      Zareeba Duviya|            37|   47297.2972972973|\n|      Madinat Hind 2|            27| 252629.62962962964|\n|       Al Yelayiss 1|         14109| 130574.46580197038|\n|       Saih Shuaib 2|         53945|  561362.1079988878|\n|    Al Qusais Second|         43591|  782021.3487417127|\n|Al Goze Industria...|         71385|  525690.9816768229|\n|     Ghadeer Barashy|             2|           192726.5|\n|Al Barsha South F...|        138881| 2716355.5188470706|\n|        Business Bay|        219506|  613123.3358541452|\n+--------------------+--------------+-------------------+\nonly showing top 20 rows\n\n+--------------------+--------------------+--------------------+-------------------+\n| nearest_landmark_en|    nearest_metro_en|     nearest_mall_en|avg_contract_amount|\n+--------------------+--------------------+--------------------+-------------------+\n|      Global Village|                    |                    | 194200.61215829468|\n|Dubai Internation...|Al Jadaf Metro St...|          Dubai Mall| 326041.46007762686|\n|Dubai Internation...|Salah Al Din Metr...|          Dubai Mall|  371619.3205636664|\n|           Jabel Ali|UAE Exchange Metr...|                    | 106021.69506726458|\n|        Burj Khalifa|Burjuman Metro St...|          Dubai Mall|   79838.0288378144|\n|Dubai Internation...|STADIUM Metro Sta...|                    | 137928.44276589272|\n|                    |                    |                    |  681013.2932913604|\n|IMG World Adventures| Creek Metro Station|                    | 260369.92753623187|\n|      Expo 2020 Site|UAE Exchange Metr...|  Ibn-e-Battuta Mall| 1604138.8980627635|\n|Sports City Swimm...|Ibn Battuta Metro...|  Ibn-e-Battuta Mall| 225232.85012195926|\n|        Burj Al Arab|Jumeirah Beach Re...|         Marina Mall|  2285887.303643632|\n|      Downtown Dubai| Creek Metro Station|          Dubai Mall|  143827.6565850223|\n|        Burj Al Arab|       Palm Jumeirah|         Marina Mall|  557258.3738131787|\n|Dubai Internation...| Creek Metro Station|          Dubai Mall| 119411.42427366448|\n|          Motor City|                    |                    |  436057.6914563001|\n|          Motor City|First Abu Dhabi B...|Mall of the Emirates|  783153.1858870335|\n|        Burj Al Arab|Noor Bank Metro S...|Mall of the Emirates|  686177.8458361548|\n|Dubai Internation...|Al Nahda Metro St...|  City Centre Mirdif|  64060.75936766077|\n|Dubai Internation...| Union Metro Station|          Dubai Mall|  459082.8912628468|\n|Dubai Internation...|Oud Metha Metro S...|          Dubai Mall|  880363.8348734981|\n+--------------------+--------------------+--------------------+-------------------+\nonly showing top 20 rows\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# prompt: 6. Tenant Insights\n# Tenant Types: Analyze the distribution of tenant types to understand the demographics of renters.\n# Contract Amounts by Tenant Type: Compare average contract amounts across different tenant types to identify potential market segments.\n\n# Tenant Insights\n# Tenant Types: Analyze the distribution of tenant types to understand the demographics of renters.\ntenant_type_distribution = rent_contracts_df.groupBy(\"tenant_type_en\").count().orderBy(\"count\", ascending=False)\ntenant_type_distribution.show()\n\n# Contract Amounts by Tenant Type: Compare average contract amounts across different tenant types to identify potential market segments.\navg_contract_by_tenant_type = rent_contracts_df.groupBy(\"tenant_type_en\").agg(avg(\"contract_amount\").alias(\"avg_contract_amount\"))\navg_contract_by_tenant_type.show()\n","metadata":{"id":"ZdcQuGLi05yS","outputId":"6576076e-6a63-4cfd-f9c0-1dffec7faf26","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:32:59.386708Z","iopub.execute_input":"2025-02-25T09:32:59.387062Z","iopub.status.idle":"2025-02-25T09:33:01.640351Z","shell.execute_reply.started":"2025-02-25T09:32:59.387028Z","shell.execute_reply":"2025-02-25T09:33:01.63896Z"}},"outputs":[{"name":"stdout","text":"+--------------+-------+\n|tenant_type_en|  count|\n+--------------+-------+\n|        Person|4268131|\n|     Authority|3480826|\n|              | 810483|\n+--------------+-------+\n\n+--------------+-------------------+\n|tenant_type_en|avg_contract_amount|\n+--------------+-------------------+\n|        Person| 208043.31137797787|\n|     Authority|  1476358.804588911|\n|              | 290825.37117003073|\n+--------------+-------------------+\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# prompt: 8. Predictive Analysis\n# Price Prediction: Use regression analysis to predict contract amounts based on features such as property type, area, and contract duration.\n# Churn Prediction: Analyze factors that may lead to tenant churn (e.g., contract renewals) to develop strategies for tenant retention.\n\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.sql.functions import col\n\n# Filter out rows where 'ejari_property_type_en' or 'area_name_en' is null or empty\ncleaned_data = rent_contracts_df.filter(\n    col('ejari_property_type_en').isNotNull() & (col('ejari_property_type_en') != \"\") &\n    col('area_name_en').isNotNull() & (col('area_name_en') != \"\")\n)\n\n# Handle categorical columns (ejari_property_type_en, area_name_en) with StringIndexer and OneHotEncoder\n\narea_indexer = StringIndexer(inputCol=\"area_name_en\", outputCol=\"area_index\")\nproperty_type_indexer = StringIndexer(inputCol=\"ejari_property_type_en\", outputCol=\"property_type_index\")\n\narea_encoder = OneHotEncoder(inputCol=\"area_index\", outputCol=\"area_encoded\")\nproperty_type_encoder = OneHotEncoder(inputCol=\"property_type_index\", outputCol=\"property_type_encoded\")\n\n# Assemble features\nassembler = VectorAssembler(inputCols=[\"area_encoded\", \"property_type_encoded\", \"contract_duration\", \"contract_amount\"], outputCol=\"features\")\n\n# StandardScaler (optional but can improve model performance)\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n\n# Regression model\nlr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"contract_amount\")\n\n# Pipeline\npipeline = Pipeline(stages=[area_indexer, property_type_indexer, area_encoder, property_type_encoder, assembler, scaler, lr])\n\n# Split the data into training and testing sets\ntrain_data, test_data = cleaned_data.randomSplit([0.8, 0.2], seed=42)\n\n# Train the model\nmodel = pipeline.fit(train_data)\n\n# Make predictions\npredictions = model.transform(test_data)\n\n# Evaluate the model\nevaluator = RegressionEvaluator(labelCol=\"contract_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")","metadata":{"id":"QeregJff1XJH","outputId":"7069eeb0-ef8b-4bfd-9bf1-93cc3ab3d2ef","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:33:01.641548Z","iopub.execute_input":"2025-02-25T09:33:01.642468Z","iopub.status.idle":"2025-02-25T09:34:23.825645Z","shell.execute_reply.started":"2025-02-25T09:33:01.64242Z","shell.execute_reply":"2025-02-25T09:34:23.82383Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-842eca012131>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o165.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 39.0 failed 1 times, most recent failure: Lost task 3.0 in stage 39.0 (TID 66) (3240f3bf19f6 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4126/0x00000008417d4040`: (struct<area_encoded:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,property_type_encoded:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,contract_duration_double_VectorAssembler_ad93cbce6720:double,contract_amount_double_VectorAssembler_ad93cbce6720:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4126/0x00000008417d4040`: (struct<area_encoded:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,property_type_encoded:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,contract_duration_double_VectorAssembler_ad93cbce6720:double,contract_amount_double_VectorAssembler_ad93cbce6720:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n"],"ename":"Py4JJavaError","evalue":"An error occurred while calling o165.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 39.0 failed 1 times, most recent failure: Lost task 3.0 in stage 39.0 (TID 66) (3240f3bf19f6 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4126/0x00000008417d4040`: (struct<area_encoded:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,property_type_encoded:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,contract_duration_double_VectorAssembler_ad93cbce6720:double,contract_amount_double_VectorAssembler_ad93cbce6720:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4126/0x00000008417d4040`: (struct<area_encoded:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,property_type_encoded:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,contract_duration_double_VectorAssembler_ad93cbce6720:double,contract_amount_double_VectorAssembler_ad93cbce6720:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n","output_type":"error"}],"execution_count":12}]}